{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get environ variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "#os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models with GPT3.5 and GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data at speeds unattainable by classical computers.\n"
     ]
    }
   ],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI() #this will use default gpt-3.5-turbo and temperature 0.7\n",
    "#llm = ChatOpenAI(model='gpt-4-turbo-preview')\n",
    "\n",
    "output = llm.invoke('Explain quantum computing in one sentence.')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum Computing nutzt die Prinzipien der Quantenmechanik, um Datenverarbeitungsaufgaben viel schneller als klassische Computer zu lösen.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import(\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a physicist.  Respond only in german.\"),\n",
    "    HumanMessage(content=\"Explain quantum computing in one sentence.\")\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching LLM Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Memory-Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.38 ms, sys: 3.85 ms, total: 13.2 ms\n",
      "Wall time: 558 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "prompt = 'Tell me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 521 µs, sys: 1.09 ms, total: 1.61 ms\n",
      "Wall time: 1.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLlite Cachain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.51 ms, sys: 4.45 ms, total: 11 ms\n",
      "Wall time: 8.98 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nPerché i pomodori non possono mai essere tristi?\\n\\nPerché sempre sono \"pomodori\" e mai \"patatini\"!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path='resources/cache.db'))\n",
    "\n",
    "# First request (not in cache, will take longer)\n",
    "llm.invoke('tell me a joke in italian')\n",
    "\n",
    "# Second request  (cached, faster)\n",
    "llm.invoke('tell me a joke in italian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Under the pale moonlight, a raven takes flight\n",
      "Across the darkened sky, soaring high\n",
      "Its black wings spread wide, a messenger of the night\n",
      "Calling out to the moon, a haunting cry\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Verse 2:\n",
      "The moon's silver glow, illuminates the land below\n",
      "The raven's piercing eyes, see what others cannot find\n",
      "Together they roam, in the shadows unknown\n",
      "A bond unbreakable, of a different kind\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Bridge:\n",
      "In the darkness they find solace, in each other's presence\n",
      "A cosmic connection, beyond mere mortal essence\n",
      "They watch over us, in the stillness of the night\n",
      "Moon and raven, forever in flight\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Outro:\n",
      "As the moon fades away, and the dawn breaks the day\n",
      "The raven takes its leave, but will return again\n",
      "To fly with the moon, in the endless sky\n",
      "A timeless bond, that will never die.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "prompt = 'write a rock song about moon and a raven'\n",
    "\n",
    "print(llm.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a experienced virologist.  Write a few sentences about the following virus SARS-CoV-2 in french.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = '''You are a experienced virologist.  Write a few sentences about the following virus {virus} in {language}.\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='SARS-CoV-2', language='french')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le virus SARS-CoV-2, responsable de la maladie COVID-19, est un virus à ARN de la famille des coronavirus. Il a émergé en Chine en décembre 2019 et s'est rapidement propagé à travers le monde, provoquant une pandémie. Ce virus peut entraîner des symptômes légers à sévères, notamment des difficultés respiratoires et une pneumonie. Il est essentiel de suivre les mesures de prévention telles que le port du masque, la distanciation sociale et le lavage fréquent des mains pour limiter sa propagation.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in json format'), HumanMessage(content='Top 5 countries in World by population')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in json format'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n=5, area='World')\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": \"China\",\n",
      "    \"2\": \"India\",\n",
      "    \"3\": \"United States\",\n",
      "    \"4\": \"Indonesia\",\n",
      "    \"5\": \"Pakistan\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a experienced virologist.  Write a few sentences about the following virus SARS-CoV-2 in french.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = '''You are a experienced virologist.  Write a few sentences about the following virus {virus} in {language}.'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus': 'SARS-CoV-2', 'language': 'french'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virus': 'SARS-CoV-2', 'language': 'french', 'text': \"Le SARS-CoV-2 est un virus de la famille des coronavirus qui a émergé en Chine en décembre 2019. Il est responsable de la maladie respiratoire COVID-19, qui peut entraîner des symptômes légers à graves, voire la mort chez les personnes vulnérables. Ce virus a provoqué une pandémie mondiale en 2020, mettant à rude épreuve les systèmes de santé et entraînant des mesures de confinement et de distanciation sociale à l'échelle internationale.\"}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = 'What is the capital of {country}? List the top 3 places to visit in that city. Use bullet points.'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter a country: ')\n",
    "output = chain.invoke({'country': country})\n",
    "print(output['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template = 'You are an experienced scientist and Python programmer.  Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "llm2 = ChatOpenAI(model='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template = 'Given the the Python function {function}, describe it as detailed as possible'\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)    \n",
    "\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains = [chain1, chain2], verbose=True)\n",
    "\n",
    "output = overall_chain.invoke('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Agents in Action: Python REPL\n",
    "### Read-Eval-Print Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[13, 26, 39, 52, 65, 78, 91]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "python_repl.run('print([n for n in range (1,100) if n % 13 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo solve this, I will first calculate the factorial of 12 using the `math.factorial` function. Then, I will calculate the square root of that result using `math.sqrt`. Finally, I will format the result to display it with 4 decimal points using the `format` function.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that the math module is imported, I can calculate the factorial of 12.\n",
      "Action: Python_REPL\n",
      "Action Input: factorial_12 = math.factorial(12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have calculated the factorial of 12, I will calculate its square root.\n",
      "Action: Python_REPL\n",
      "Action Input: sqrt_factorial_12 = math.sqrt(factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have the square root of the factorial of 12, I will format it to display with 4 decimal points.\n",
      "Action: Python_REPL\n",
      "Action Input: print(f\"{sqrt_factorial_12:.4f}\")\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 21886.1052\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of the factorial 12 and display it with 4 decimal points',\n",
       " 'output': '21886.1052'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm, \n",
    "    tool=PythonREPLTool(), \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.invoke('Calculate the square root of the factorial 12 and display it with 4 decimal points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate 5.1 raised to the power of 7.3 to get the answer.\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the answer to 5.1 ** 7.3', 'output': '146306.05007233328'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the answer to 5.1 ** 7.3\n",
      "146306.05007233328\n"
     ]
    }
   ],
   "source": [
    "# this the value of response:\n",
    "# {'input': 'What is the answer to 5.1 ** 7.3', 'output': '146306.05007233328'}\n",
    "\n",
    "print(response['input'])\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Tool: DuckDuckGo and Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freddie Mercury was born Farrokh Bulsara in Stone Town, in the British protectorate of Zanzibar (now part of Tanzania), on September 5, 1946. His first big challenge was to come to terms with... But there are many lesser-known Freddie Mercury facts that reveal hidden depths to the life and work of the man born Farrokh Bulsara in Zanzibar, on September 5, 1946. Here are 15 surprising... Although he was born on the East African island of Zanzibar on September 5, 1946, his parents were both Indian; they were Parsees from Gujarat. Mercury would often make note of his Persian roots, but the Parsees had been driven from Persia a thousand years before. They landed on the Gujarat coast and populated its greatest city, Bombay. Table of Contents Freddie Mercury was born on September 5, 1946. Freddie Mercury, whose real name was Farrokh Bulsara, was born in Zanzibar, Tanzania. He later became the lead vocalist of the legendary rock band Queen. Freddie Mercury had a four-octave vocal range. Freddie Mercury was born in Tanzania, schooled in India, and moved to the UK. But whatever his roots may be, Mercury became and remains an international superstar. As the man once said, \"I am not going to be a rock star. I am going to be a legend.\" And yeah, he totally fulfilled his promise, but while he lived like a legend, he died an enigma.\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun \n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "output = search.invoke('Where was Freddie Mercury born?')\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: From his earliest songs with Queen, Mercury built a reputation as one of the most popular songwriters of his generation, and the best Freddie Mercury songs reveal an artist committed to..., title: Best Freddie Mercury Songs: 20 Essential Solo And Queen Tracks, link: https://www.udiscovermusic.com/stories/best-freddie-mercury-songs/], [snippet: Everybody knows that Freddie Mercury was a gifted songwriter, a breathtakingly original performer, and frontman of one of the greatest bands of all time, Queen. But there are many..., title: Freddie Mercury Facts: Things You Never Knew About Queen's Frontman, link: https://www.udiscovermusic.com/stories/freddie-mercury-facts-you-need-to-know/], [snippet: Ex-fiancée explains decision Who is Mary Austin? Meet the woman Freddie Mercury asked to marry Freddie Mercury facts: Queen singer's age, teeth, real name, relationships and more explained Freddie Mercury in private: 20 rarely seen photos of the star behind closed doors, title: Freddie Mercury: Mary Austin breaks silence on relationship with Queen ..., link: https://www.smoothradio.com/artists/freddie-mercury/mary-austin-interview-relationship/], [snippet: May 31, 2023 Leer en español It's one of rock's best-known and strangest songs: a six-minute radio hit that starts out as a piano ballad, becomes a high-pitched opera, then tumbles into a..., title: Was Queen's 'Bohemian Rhapsody' First ... - The New York Times, link: https://www.nytimes.com/2023/05/31/arts/music/queen-freddie-mercury-bohemian-rhapsody.html]\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "output = search.run('Freddie Mercury and Queen')\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region = 'de-de', max_results = 3, safesearch = 'moderate')\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source = 'news')\n",
    "\n",
    "output=search.run('Berlin')\n",
    "\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get environ variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "#os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models with GPT3.5 and GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data at speeds unattainable by classical computers.\n"
     ]
    }
   ],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI() #this will use default gpt-3.5-turbo and temperature 0.7\n",
    "#llm = ChatOpenAI(model='gpt-4-turbo-preview')\n",
    "\n",
    "output = llm.invoke('Explain quantum computing in one sentence.')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum Computing nutzt die Prinzipien der Quantenmechanik, um Datenverarbeitungsaufgaben viel schneller als klassische Computer zu lösen.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import(\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a physicist.  Respond only in german.\"),\n",
    "    HumanMessage(content=\"Explain quantum computing in one sentence.\")\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching LLM Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Memory-Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.38 ms, sys: 3.85 ms, total: 13.2 ms\n",
      "Wall time: 558 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "prompt = 'Tell me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 521 µs, sys: 1.09 ms, total: 1.61 ms\n",
      "Wall time: 1.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLlite Cachain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.51 ms, sys: 4.45 ms, total: 11 ms\n",
      "Wall time: 8.98 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nPerché i pomodori non possono mai essere tristi?\\n\\nPerché sempre sono \"pomodori\" e mai \"patatini\"!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path='resources/cache.db'))\n",
    "\n",
    "# First request (not in cache, will take longer)\n",
    "llm.invoke('tell me a joke in italian')\n",
    "\n",
    "# Second request  (cached, faster)\n",
    "llm.invoke('tell me a joke in italian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Under the pale moonlight, a raven takes flight\n",
      "Across the darkened sky, soaring high\n",
      "Its black wings spread wide, a messenger of the night\n",
      "Calling out to the moon, a haunting cry\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Verse 2:\n",
      "The moon's silver glow, illuminates the land below\n",
      "The raven's piercing eyes, see what others cannot find\n",
      "Together they roam, in the shadows unknown\n",
      "A bond unbreakable, of a different kind\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Bridge:\n",
      "In the darkness they find solace, in each other's presence\n",
      "A cosmic connection, beyond mere mortal essence\n",
      "They watch over us, in the stillness of the night\n",
      "Moon and raven, forever in flight\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Outro:\n",
      "As the moon fades away, and the dawn breaks the day\n",
      "The raven takes its leave, but will return again\n",
      "To fly with the moon, in the endless sky\n",
      "A timeless bond, that will never die.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "prompt = 'write a rock song about moon and a raven'\n",
    "\n",
    "print(llm.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a experienced virologist.  Write a few sentences about the following virus SARS-CoV-2 in french.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = '''You are a experienced virologist.  Write a few sentences about the following virus {virus} in {language}.\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='SARS-CoV-2', language='french')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le virus SARS-CoV-2, responsable de la maladie COVID-19, est un virus à ARN de la famille des coronavirus. Il a émergé en Chine en décembre 2019 et s'est rapidement propagé à travers le monde, provoquant une pandémie. Ce virus peut entraîner des symptômes légers à sévères, notamment des difficultés respiratoires et une pneumonie. Il est essentiel de suivre les mesures de prévention telles que le port du masque, la distanciation sociale et le lavage fréquent des mains pour limiter sa propagation.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in json format'), HumanMessage(content='Top 5 countries in World by population')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in json format'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n=5, area='World')\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": \"China\",\n",
      "    \"2\": \"India\",\n",
      "    \"3\": \"United States\",\n",
      "    \"4\": \"Indonesia\",\n",
      "    \"5\": \"Pakistan\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a experienced virologist.  Write a few sentences about the following virus SARS-CoV-2 in french.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = '''You are a experienced virologist.  Write a few sentences about the following virus {virus} in {language}.'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus': 'SARS-CoV-2', 'language': 'french'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virus': 'SARS-CoV-2', 'language': 'french', 'text': \"Le SARS-CoV-2 est un virus de la famille des coronavirus qui a émergé en Chine en décembre 2019. Il est responsable de la maladie respiratoire COVID-19, qui peut entraîner des symptômes légers à graves, voire la mort chez les personnes vulnérables. Ce virus a provoqué une pandémie mondiale en 2020, mettant à rude épreuve les systèmes de santé et entraînant des mesures de confinement et de distanciation sociale à l'échelle internationale.\"}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = 'What is the capital of {country}? List the top 3 places to visit in that city. Use bullet points.'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter a country: ')\n",
    "output = chain.invoke({'country': country})\n",
    "print(output['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here is a simple implementation of linear regression in Python:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(X, y):\n",
      "    # Calculate the number of observations\n",
      "    n = len(X)\n",
      "    \n",
      "    # Calculate the mean of X and y\n",
      "    mean_X = np.mean(X)\n",
      "    mean_y = np.mean(y)\n",
      "    \n",
      "    # Calculate the slope (m) and intercept (b) of the regression line\n",
      "    numer = 0\n",
      "    denom = 0\n",
      "    for i in range(n):\n",
      "        numer += (X[i] - mean_X) * (y[i] - mean_y)\n",
      "        denom += (X[i] - mean_X) ** 2\n",
      "    m = numer / denom\n",
      "    b = mean_y - m * mean_X\n",
      "    \n",
      "    return m, b\n",
      "\n",
      "# Example usage\n",
      "X = np.array([1, 2, 3, 4, 5])\n",
      "y = np.array([2, 4, 5, 4, 5])\n",
      "\n",
      "m, b = linear_regression(X, y)\n",
      "print(f\"Slope: {m}, Intercept: {b}\")\n",
      "```\n",
      "\n",
      "This function takes in two arrays `X` and `y` representing the independent and dependent variables, respectively. It calculates the slope `m` and intercept `b` of the regression line using the least squares method. Finally, it returns the slope and intercept values.\n",
      "\n",
      "You can test this function with your own data by passing in arrays `X` and `y` representing your data points.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe Python function provided is an implementation of simple linear regression, which is a fundamental statistical method used to model the linear relationship between an independent variable (X) and a dependent variable (y). The relationship is represented as a straight line with the equation `y = mX + b`.\n",
      "\n",
      "Let's walk through the function step by step:\n",
      "\n",
      "### Importing Libraries\n",
      "\n",
      "The function begins by importing `numpy`, a powerful Python library for numerical calculations. Here, it's primarily used for handling arrays (`X` and `y`) and calculating mean values.\n",
      "\n",
      "### Function Definition\n",
      "\n",
      "`linear_regression(X, y)`: This function takes two parameters:\n",
      "- `X`: An array of the independent variable.\n",
      "- `y`: An array of the dependent variable.\n",
      "\n",
      "Both `X` and `y` must be numpy arrays and of the same length.\n",
      "\n",
      "### Calculating the Mean\n",
      "\n",
      "The first step in the body of the function is to determine the number of observations `n` by getting the length of `X`. Assuming both `X` and `y` have the same number of elements, this gives the number of data points.\n",
      "\n",
      "Then, it calculates the means of `X` and `y` using `np.mean()`. These mean values (`mean_X`, `mean_y`) are crucial for the calculation of the slope `m` and intercept `b` of the regression line.\n",
      "\n",
      "### Calculating the Slope and Intercept\n",
      "\n",
      "Next, the function initializes two variables, `numer` (numerator) and `denom` (denominator), which will hold the sums needed to calculate the slope `m`.\n",
      "\n",
      "- The loop iterates over each observation, updating `numer` with the product of the difference between the current `X` value (`X[i]`) and the mean of `X` (`mean_X`) and the difference between the current `y` value (`y[i]`) and the mean of `y` (`mean_y`).\n",
      "- Concurrently, `denom` is updated with the squared difference between each `X` value and the mean of `X`.\n",
      "\n",
      "The slope `m` is then calculated as the quotient of `numer` and `denom`.\n",
      "\n",
      "The intercept `b` is determined by subtracting the product of the slope `m` and the mean of `X` from the mean of `y`.\n",
      "\n",
      "### Return Values\n",
      "\n",
      "The function returns two values:\n",
      "- `m`: The slope of the line, indicating how much `y` is expected to change with a one-unit change in `X`.\n",
      "- `b`: The intercept of the line, indicating the value of `y` when `X` is 0.\n",
      "\n",
      "These two values define the best-fitting line that minimizes the sum of squared differences between the observed values and the values predicted by the linear model.\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "Finally, an example usage of the function `linear_regression()` is given with sample data. This demonstrates calculating the slope and intercept from the given `X` and `y` arrays, followed by printing these values. \n",
      "\n",
      "The particularity of this function is that it follows the classical approach to calculating linear regression parameters, directly implementing the mathematical formulas without relying on higher-level libraries specialized in machine learning or data analysis, making it perfect for educational purposes or understanding the underlying mechanics of linear regression.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template = 'You are an experienced scientist and Python programmer.  Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "llm2 = ChatOpenAI(model='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template = 'Given the the Python function {function}, describe it as detailed as possible'\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)    \n",
    "\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains = [chain1, chain2], verbose=True)\n",
    "\n",
    "output = overall_chain.invoke('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get environ variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "#os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models with GPT3.5 and GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data at speeds unattainable by classical computers.\n"
     ]
    }
   ],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI() #this will use default gpt-3.5-turbo and temperature 0.7\n",
    "#llm = ChatOpenAI(model='gpt-4-turbo-preview')\n",
    "\n",
    "output = llm.invoke('Explain quantum computing in one sentence.')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum Computing nutzt die Prinzipien der Quantenmechanik, um Datenverarbeitungsaufgaben viel schneller als klassische Computer zu lösen.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import(\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a physicist.  Respond only in german.\"),\n",
    "    HumanMessage(content=\"Explain quantum computing in one sentence.\")\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching LLM Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Memory-Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.38 ms, sys: 3.85 ms, total: 13.2 ms\n",
      "Wall time: 558 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "prompt = 'Tell me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 521 µs, sys: 1.09 ms, total: 1.61 ms\n",
      "Wall time: 1.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLlite Cachain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.51 ms, sys: 4.45 ms, total: 11 ms\n",
      "Wall time: 8.98 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nPerché i pomodori non possono mai essere tristi?\\n\\nPerché sempre sono \"pomodori\" e mai \"patatini\"!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path='resources/cache.db'))\n",
    "\n",
    "# First request (not in cache, will take longer)\n",
    "llm.invoke('tell me a joke in italian')\n",
    "\n",
    "# Second request  (cached, faster)\n",
    "llm.invoke('tell me a joke in italian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Under the pale moonlight, a raven takes flight\n",
      "Across the darkened sky, soaring high\n",
      "Its black wings spread wide, a messenger of the night\n",
      "Calling out to the moon, a haunting cry\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Verse 2:\n",
      "The moon's silver glow, illuminates the land below\n",
      "The raven's piercing eyes, see what others cannot find\n",
      "Together they roam, in the shadows unknown\n",
      "A bond unbreakable, of a different kind\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Bridge:\n",
      "In the darkness they find solace, in each other's presence\n",
      "A cosmic connection, beyond mere mortal essence\n",
      "They watch over us, in the stillness of the night\n",
      "Moon and raven, forever in flight\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "Their spirits intertwined, a mystical sight\n",
      "Silent watchers of the world, hidden from plain sight\n",
      "Moon and raven, guardians of the night\n",
      "\n",
      "Outro:\n",
      "As the moon fades away, and the dawn breaks the day\n",
      "The raven takes its leave, but will return again\n",
      "To fly with the moon, in the endless sky\n",
      "A timeless bond, that will never die.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "prompt = 'write a rock song about moon and a raven'\n",
    "\n",
    "print(llm.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a experienced virologist.  Write a few sentences about the following virus SARS-CoV-2 in french.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = '''You are a experienced virologist.  Write a few sentences about the following virus {virus} in {language}.\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='SARS-CoV-2', language='french')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le virus SARS-CoV-2, responsable de la maladie COVID-19, est un virus à ARN de la famille des coronavirus. Il a émergé en Chine en décembre 2019 et s'est rapidement propagé à travers le monde, provoquant une pandémie. Ce virus peut entraîner des symptômes légers à sévères, notamment des difficultés respiratoires et une pneumonie. Il est essentiel de suivre les mesures de prévention telles que le port du masque, la distanciation sociale et le lavage fréquent des mains pour limiter sa propagation.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in json format'), HumanMessage(content='Top 5 countries in World by population')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in json format'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n=5, area='World')\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": \"China\",\n",
      "    \"2\": \"India\",\n",
      "    \"3\": \"United States\",\n",
      "    \"4\": \"Indonesia\",\n",
      "    \"5\": \"Pakistan\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a experienced virologist.  Write a few sentences about the following virus SARS-CoV-2 in french.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = '''You are a experienced virologist.  Write a few sentences about the following virus {virus} in {language}.'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus': 'SARS-CoV-2', 'language': 'french'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virus': 'SARS-CoV-2', 'language': 'french', 'text': \"Le SARS-CoV-2 est un virus de la famille des coronavirus qui a émergé en Chine en décembre 2019. Il est responsable de la maladie respiratoire COVID-19, qui peut entraîner des symptômes légers à graves, voire la mort chez les personnes vulnérables. Ce virus a provoqué une pandémie mondiale en 2020, mettant à rude épreuve les systèmes de santé et entraînant des mesures de confinement et de distanciation sociale à l'échelle internationale.\"}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = 'What is the capital of {country}? List the top 3 places to visit in that city. Use bullet points.'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter a country: ')\n",
    "output = chain.invoke({'country': country})\n",
    "print(output['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template = 'You are an experienced scientist and Python programmer.  Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "\n",
    "chain1 = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
